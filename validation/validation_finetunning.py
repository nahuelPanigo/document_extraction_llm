import os
import json
from pathlib import Path
import sys
from typing import Optional, Dict, Any
from dotenv import load_dotenv
import time

sys.path.append(str(Path(__file__).resolve().parents[1]))

from constants import (
    RESULT_FOLDER_VALIDATION,
    PDF_FOLDER,
    ROOT_DIR
)
from utils.text_extraction.read_and_write_files import read_data_json, write_to_json
from utils.consume_apis.consume_orchestrator import upload_file


class FinetunedValidator:
    """
    Validator using local fine-tuned orchestrator service
    Calls the orchestrator service to extract metadata from PDFs
    """

    def __init__(self, orchestrator_url: str = None, token: str = None, ocr: bool = False):
        """
        Initialize the Finetuned Validator

        Args:
            orchestrator_url: URL of orchestrator service (default: http://localhost:8000)
            token: Bearer token for authentication
            ocr: Enable OCR for image extraction
        """
        load_dotenv()

        self.orchestrator_url = orchestrator_url or os.getenv("ORCHESTRATOR_URL", "http://localhost:8000")
        self.token = token or os.getenv("ORCHESTRATOR_TOKEN")
        self.ocr = ocr

        if not self.token:
            raise ValueError("ORCHESTRATOR_TOKEN environment variable is required or pass token parameter")

        self.results_folder = RESULT_FOLDER_VALIDATION / "FINETUNNED"
        self.results_folder.mkdir(parents=True, exist_ok=True)

        print(f"‚úÖ Initialized Finetuned Validator")
        print(f"üîó Orchestrator URL: {self.orchestrator_url}")
        print(f"üîç OCR enabled: {self.ocr}")

    def process_document(self, doc_id: str, pdf_path: Path, normalization: bool = True,
                        deepanalyze: bool = False, doc_type: str = "None") -> Optional[Dict[str, Any]]:
        """
        Process a single document through the orchestrator service

        Args:
            doc_id: Document identifier
            pdf_path: Path to PDF file
            normalization: Apply text normalization
            deepanalyze: Enable deep analysis
            doc_type: Document type (Articulo, Libro, Tesis, etc.)

        Returns:
            Extracted metadata or None if error
        """
        print(f"\nüîç Processing document: {doc_id}")

        try:
            # Call orchestrator service
            print(f"üì§ Sending {pdf_path.name} to orchestrator service...")
            response = upload_file(
                file_path=pdf_path,
                token=self.token,
                normalization=normalization,
                type=doc_type,
                deepanalyze=deepanalyze,
                host_url=self.orchestrator_url,
                ocr=self.ocr
            )

            # Debug: print raw response
            print(f"üì• Raw response type: {type(response)}")
            print(f"üì• Raw response: {response}")

            # Check if request was successful
            if not response:
                print(f"‚ùå Empty response from orchestrator for {doc_id}")
                return None

            # Check for errors in response
            if not response.get("success", False):
                error_msg = response.get("error", {}).get("message", "Unknown error")
                error_code = response.get("error", {}).get("code", "Unknown code")
                print(f"‚ùå Orchestrator error for {doc_id}: {error_msg} (code: {error_code})")
                return None

            # Extract metadata from response
            metadata = response.get("data", {})

            if not metadata:
                print(f"‚ö†Ô∏è No metadata extracted for {doc_id}")
                return None

            # Return metadata directly without wrapper keys
            print(f"‚úÖ Successfully processed {doc_id}")
            print(f"üìä Extracted fields: {list(metadata.keys())}")

            return metadata

        except Exception as e:
            print(f"‚ùå Error processing {doc_id}: {str(e)}")
            return None

    def save_results(self, results: Dict[str, Any], filename: str):
        """Save results to JSON file"""
        output_path = self.results_folder / filename
        try:
            write_to_json(output_path, results, 'utf-8')
            print(f"üíæ Results saved to: {output_path}")
        except Exception as e:
            print(f"‚ùå Error saving results: {str(e)}")


def main():
    """Main function to run the validation process"""
    load_dotenv()

    # Configuration
    orchestrator_url = os.getenv("ORCHESTRATOR_URL", "http://localhost:8000")
    token = os.getenv("ORCHESTRATOR_TOKEN")
    ocr = os.getenv("OCR_ENABLED", "false").lower() == "true"
    normalization = os.getenv("NORMALIZATION_ENABLED", "true").lower() == "true"
    deepanalyze = os.getenv("DEEPANALYZE_ENABLED", "false").lower() == "true"

    print(f"üöÄ Starting Finetuned Model Validation")
    print("=" * 60)

    try:
        # Initialize validator
        validator = FinetunedValidator(
            orchestrator_url=orchestrator_url,
            token=token,
            ocr=ocr
        )

        # Read JSON file
        #json_file_path = RESULT_FOLDER_VALIDATION / "test_metadata_validada.json"

        json_file_path = RESULT_FOLDER_VALIDATION / "test_metadata_to_validate.json"

        try:
            data = read_data_json(json_file_path, 'utf-8')
        except Exception as e:
            print(f"‚ùå Error reading JSON file: {str(e)}")
            return

        if not data:
            print("‚ùå No data found in JSON file")
            return

        print(f"üìã Found {len(data)} documents to process")

        # Process each document
        results = {}
        successful_count = 0

        for doc_id, metadata in data.items():
            if not doc_id:
                continue

            # Construct PDF filename
            pdf_filename = f"{doc_id}.pdf"
            pdf_path = PDF_FOLDER / pdf_filename

            print(f"üîç Looking for PDF: {pdf_path}")
            print(f"   PDF_FOLDER: {PDF_FOLDER}")
            print(f"   doc_id: {doc_id}")
            print(f"   pdf_filename: {pdf_filename}")
            print(f"   Path exists: {pdf_path.exists()}")

            if not pdf_path.exists():
                print(f"‚ö†Ô∏è PDF file not found: {pdf_path}")
                continue

            # Extract document type from original metadata
            doc_type = metadata.get("type", metadata.get("dc.type", "None"))

            # Process document
            result = validator.process_document(
                doc_id=doc_id,
                pdf_path=pdf_path,
                normalization=normalization,
                deepanalyze=deepanalyze,
                doc_type=doc_type
            )

            if result:
                results[doc_id] = result
                successful_count += 1

        # Save results
        if results:
            timestamp = int(time.time())
            ocr_suffix = "_ocr" if ocr else ""
            deepanalyze_suffix = "_deepanalyze" if deepanalyze else ""
            filename = f"finetuned_validation{ocr_suffix}{deepanalyze_suffix}_{timestamp}_with_date_fixed.json"
            validator.save_results(results, filename)

            print(f"\n‚úÖ Validation completed!")
            print(f"üìä Successfully processed: {successful_count}/{len(data)} documents")
            print(f"üíæ Results saved to: {validator.results_folder / filename}")
        else:
            print("‚ùå No documents were successfully processed")

    except Exception as e:
        print(f"‚ùå Critical error: {str(e)}")
        raise


if __name__ == "__main__":
    main()
